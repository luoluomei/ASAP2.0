# -*- coding: utf-8 -*-
"""run_baseline2_dynamicrubric.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bssWtZK2sjHLihWKJs0BR_0doS5P5FPJ
"""

# -*- coding: utf-8 -*-
"""
Baseline-2 (MTS) â€” Dynamic Rubric Version
Step A: è®© LLM åŸºäºå®˜æ–¹ Holistic Rubric ç”Ÿæˆä¸‰ç»´åº¦(Dev/Org/Lang) çš„ 1â€“6 çº§é”šç‚¹(JSON)ï¼›ç¼“å­˜ã€‚
Step B: ç”¨â€œç”Ÿæˆç‰ˆæ ‡å‡†â€å¯¹æ¯ç¯‡ä½œæ–‡è¿›è¡Œä¸‰ç»´åº¦æ‰“åˆ†ï¼›ä¿æŒå…¶å®ƒé€»è¾‘ä¸å˜ï¼ˆè§£æã€ä¿å­˜ã€MOCK å…¼å®¹ï¼‰ã€‚
"""

import os
import re
import json
import time
import hashlib
from typing import Dict, Any

from common import (
    target_df, essay_col, score_col, id_col,
    call_llm, save_results, PROVIDER, HOLISTIC_RUBRIC_FULL
)

# ---------- Fallbackï¼šè‹¥åŠ¨æ€ç”Ÿæˆå¤±è´¥å°±ä½¿ç”¨ä¸€ä¸ªå†…ç½®çš„å¯ç”¨ MTS ç»†åŒ–ç‰ˆ ----------
DEFAULT_MTS_RUBRIC = {
    "development": {
        "1": "No viable point of view; little or no source-based evidence; reasoning absent.",
        "2": "Vague/seriously limited POV; insufficient/inappropriate evidence; weak reasoning.",
        "3": "Developing POV; inconsistent reasoning; limited/partially relevant source evidence.",
        "4": "Adequate POV; competent reasoning; adequate (sometimes generic) source evidence.",
        "5": "Clear POV; strong reasoning; mostly relevant/specific source evidence supports claims.",
        "6": "Insightful POV; outstanding reasoning; specific and well-integrated source evidence."
    },
    "organization": {
        "1": "Disorganized/unfocused; ideas largely disjointed; little discernible structure.",
        "2": "Poorly organized; serious problems with coherence/progression; hard to follow.",
        "3": "Limited organization/focus; noticeable lapses; sequencing unclear in places.",
        "4": "Generally organized/focused; some coherence and progression; simple transitions.",
        "5": "Clearly organized/focused; coherent development; minor lapses do not impede flow.",
        "6": "Well-organized, clear focus; logical paragraphing; smooth progression/c cohesion."
    },
    "language": {
        "1": "Severe flaws in sentence structure; pervasive grammar/usage/mechanics errors.",
        "2": "Very limited vocabulary or incorrect word choice; frequent sentence problems; many errors.",
        "3": "Often weak/inappropriate word choice; limited variety; noticeable sentence problems; error accumulation.",
        "4": "Generally appropriate vocabulary; some sentence variety; errors present but meaning intact.",
        "5": "Appropriate vocabulary; noticeable sentence variety; generally few errors.",
        "6": "Precise, skillful language; apt vocabulary; meaningful variety; free of most errors."
    }
}


def _normalize_json_str(s: str) -> str:
    # å°½é‡ä»æ¨¡å‹è¾“å‡ºä¸­æŠ½å–æœ€å¤–å±‚ JSON
    # 1) ç›´æ¥å°è¯• json.loads
    try:
        obj = json.loads(s)
        return json.dumps(obj, ensure_ascii=False)
    except Exception:
        pass
    # 2) æ­£åˆ™æ‰¾å‡ºæœ€å¤–å±‚ { ... } ç‰‡æ®µ
    m = re.search(r"\{[\s\S]*\}$", s.strip())
    if m:
        cand = m.group(0)
        try:
            obj = json.loads(cand)
            return json.dumps(obj, ensure_ascii=False)
        except Exception:
            pass
    # 3) ä¿é™©èµ·è§ï¼Œå†æ‰¾ç¬¬ä¸€ä¸ªå·¦å¤§æ‹¬å·åˆ°æœ€åä¸€ä¸ªå³å¤§æ‹¬å·
    first = s.find("{")
    last = s.rfind("}")
    if first != -1 and last != -1 and last > first:
        cand = s[first:last+1]
        try:
            obj = json.loads(cand)
            return json.dumps(obj, ensure_ascii=False)
        except Exception:
            pass
    raise ValueError("No valid JSON found in model output.")


def _hash_obj(obj: Dict[str, Any]) -> str:
    norm = json.dumps(obj, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(norm.encode("utf-8")).hexdigest()[:10]


def generate_trait_rubric(
    holistic_text: str,
    cache_path: str = "generated_mts_rubric.json",
    force_regen: bool = False,
    temperature: float = 0.0,
    max_tokens: int = 1200
) -> Dict[str, Dict[str, str]]:
    """
    è®© LLM æŠŠå®˜æ–¹ Holistic Rubric è½¬å†™ä¸ºä¸‰ç»´åº¦(Dev/Org/Lang) * 1â€“6 çš„é”šç‚¹æè¿°ï¼ˆJSONï¼‰ã€‚
    JSON schemaï¼ˆä¸¥æ ¼è¦æ±‚ä¸‰é”® development/organization/languageï¼Œæ¯ä¸ªé”®å« 1..6 æ–‡æœ¬ï¼‰ï¼š
    {
      "development": {"1": "...", "2": "...", ..., "6": "..."},
      "organization": {"1": "...", ..., "6": "..."},
      "language": {"1": "...", ..., "6": "..."}
    }
    """
    if (not force_regen) and os.path.exists(cache_path):
        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                cached = json.load(f)
            # åŸºæœ¬æ ¡éªŒ
            for k in ("development", "organization", "language"):
                assert k in cached and all(str(i) in cached[k] for i in range(1, 7))
            print(f"[Rubric] Loaded cached rubric from {cache_path}")
            return cached
        except Exception:
            print("[Rubric] Cache exists but invalid, will regenerate...")

    # æ„é€ æç¤ºè¯
    prompt = f"""You are an expert writing-assessment designer.
Convert the OFFICIAL HOLISTIC RUBRIC into a three-trait, 1â€“6 anchored rubric
with crisp, decisionable descriptions for each level.

Return STRICT JSON ONLY (no commentary), schema:
{{
  "development": {{"1": "...","2":"...","3":"...","4":"...","5":"...","6":"..."}},
  "organization": {{"1": "...","2":"...","3":"...","4":"...","5":"...","6":"..."}},
  "language":    {{"1": "...","2":"...","3":"...","4":"...","5":"...","6":"..."}}
}}

Rules:
- Each level (1..6) must be 1â€“2 sentences, concrete and judgeable from text.
- Avoid generic words alone; include observable signals (e.g., source-use specificity, cohesion markers, sentence control).
- Keep concise. NO extra keys. NO markdown.

=== OFFICIAL HOLISTIC RUBRIC ===
{holistic_text}
"""
    raw, _tok = call_llm(prompt, max_tokens=max_tokens, temperature=temperature)
    try:
        norm = _normalize_json_str(raw)
        obj = json.loads(norm)
        # æ ¡éªŒ key å®Œæ•´
        for k in ("development", "organization", "language"):
            if k not in obj:
                raise ValueError(f"Missing key: {k}")
            for i in range(1, 7):
                if str(i) not in obj[k]:
                    raise ValueError(f"Missing level '{i}' in {k}")
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
        print(f"[Rubric] Generated & cached to {cache_path}")
        return obj
    except Exception as e:
        print(f"[Rubric] âŒ Generation failed: {e}. Fallback to DEFAULT_MTS_RUBRIC.")
        return DEFAULT_MTS_RUBRIC


def rubric_json_to_prompt_text(rj: Dict[str, Dict[str, str]]) -> str:
    # å°† JSON è½¬ä¸ºè¯„åˆ†æç¤ºå¯è¯»æ–‡æœ¬ï¼ˆä¸¥æ ¼ä¸‰ç»´ï¼Œæ¯ç»´ 1â†’6ï¼‰
    def block(name_show: str, key: str) -> str:
        lines = [f"{name_show} (1â€“6):"]
        for i in range(1, 7):
            desc = rj.get(key, {}).get(str(i), "").strip()
            lines.append(f"- Score {i}: {desc}")
        return "\n".join(lines)

    dev = block("TRAIT 1: DEVELOPMENT (Ideas, Evidence, Critical Thinking)", "development")
    org = block("TRAIT 2: ORGANIZATION (Structure, Coherence, Focus)", "organization")
    lng = block("TRAIT 3: LANGUAGE (Vocabulary, Sentence Control, Mechanics)", "language")
    return dev + "\n\n" + org + "\n\n" + lng


def _parse_score_line(raw: str, key: str, default_val: int = 3) -> int:
    m = re.search(rf"{key}\s*:\s*(\d+)", raw, flags=re.IGNORECASE)
    if not m:
        return default_val
    try:
        v = int(m.group(1))
        return max(1, min(6, v))
    except Exception:
        return default_val


def run():
    print(f"\nğŸš€ Baseline 2 (MTS with Dynamic Rubric), provider={PROVIDER}")
    # Step A: ç”Ÿæˆ/åŠ è½½ä¸‰ç»´åº¦ 1â€“6 é”šç‚¹ rubric
    rubric = generate_trait_rubric(
        holistic_text=HOLISTIC_RUBRIC_FULL,
        cache_path="generated_mts_rubric.json",
        force_regen=False,          # éœ€è¦å¼ºåˆ¶é‡ç”Ÿï¼ŒæŠŠè¿™é‡Œæ”¹ True
        temperature=0.0,
        max_tokens=1200
    )
    rubric_id = _hash_obj(rubric)
    rubric_prompt_text = rubric_json_to_prompt_text(rubric)
    print(f"[Rubric] rubric_id={rubric_id} (will be attached to outputs)")

    results = []
    for idx, row in target_df.iterrows():
        prompt = f"""System: Expert Multi-Trait Scorer.

### RUBRIC (Generated from official holistic rubric; 1â€“6 anchors)
{rubric_prompt_text}

### TARGET ESSAY
{row[essay_col]}

### TASK
Score the 3 traits INDEPENDENTLY (integers 1â€“6). Output EXACTLY:
Development: [1-6]
Organization: [1-6]
Language: [1-6]
"""
        start_t = time.time()
        raw_out, in_tok = call_llm(prompt, max_tokens=280, temperature=0.0)
        latency = time.time() - start_t

        try:
            d = _parse_score_line(raw_out, "Development", 3)
            o = _parse_score_line(raw_out, "Organization", 3)
            l = _parse_score_line(raw_out, "Language", 3)
            final = int(round((d + o + l) / 3 + 1e-6))
            scores = {'d': d, 'o': o, 'l': l}
        except Exception:
            final = None
            scores = {'d': None, 'o': None, 'l': None}

        results.append({
            'essay_id': row[id_col],
            'human_score': row[score_col],
            'pred_score': final,
            'score_dev': scores['d'],
            'score_org': scores['o'],
            'score_lang': scores['l'],
            'rubric_id': rubric_id,
            'rubric_file': "generated_mts_rubric.json" if os.path.exists("generated_mts_rubric.json") else "DEFAULT",
            'input_tokens': in_tok,
            'latency': round(latency, 2),
            'raw_output': raw_out
        })

        if (idx + 1) % 10 == 0:
            print(f"Processed {idx + 1}/{len(target_df)}...")
        if PROVIDER != "MOCK":
            time.sleep(1)

    # ä»ç„¶å¤ç”¨ä½ çš„ save_resultsï¼ˆå®ƒä¼šè‡ªåŠ¨è®¡ç®— QWK å¹¶è½ç›˜ CSV/TXTï¼‰
    save_results(results, "Baseline2_MTS_DynRubric")


if __name__ == "__main__":
    run()