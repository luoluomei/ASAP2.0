# -*- coding: utf-8 -*-
"""run_baseline3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-39vUwo1YR1edVH6yBA_rt0X8jOs6ty
"""

import time
import numpy as np
import choix
from sklearn.linear_model import LinearRegression
from common import (
    target_df, train_df, essay_col, score_col, id_col,
    call_llm, save_results, PROVIDER, HOLISTIC_RUBRIC_FULL
)

def run():
    print(f"\nğŸš€ Starting Baseline 3 (Pairwise Full Context)...")
    print(f"âš ï¸ WARNING: This will make {len(target_df) * 6} API calls!")

    # 1. é”šç‚¹å‡†å¤‡ (æ— æˆªæ–­)
    anchors = {}
    for s in range(1, 7):
        cand = train_df[train_df[score_col] == s]
        if not cand.empty:
            # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨å®Œæ•´æ–‡æœ¬
            anchors[s] = {'id': cand.iloc[0][id_col], 'text': str(cand.iloc[0][essay_col])}
        else:
            anchors[s] = {'id': f"mock_{s}", 'text': "Mock Full Text"}

    comparisons = []
    all_ids = list(target_df[id_col].unique()) + [a['id'] for a in anchors.values()]
    all_ids = list(set(all_ids))
    id_map = {uid: i for i, uid in enumerate(all_ids)}

    # ç»Ÿè®¡å­—å…¸
    latency_map = {}
    token_map = {}

    print("Collecting comparisons...")
    for idx, row in target_df.iterrows():
        t_id = row[id_col]
        essay_start = time.time()
        essay_tokens = 0

        for s, anchor in anchors.items():
            a_id = anchor['id']
            if t_id == a_id: continue

            # ğŸ”¥ Prompt: ä½¿ç”¨å®Œæ•´ Rubric å’Œ å®Œæ•´ A/B ä½œæ–‡
            prompt = f"""System: Compare based on Official Rubric.

### RUBRIC
{HOLISTIC_RUBRIC_FULL}

### ESSAY A
{str(row[essay_col])}

### ESSAY B
{anchor['text']}

### TASK
Which essay better demonstrates the rubric's mastery? Output ONLY "Essay A" or "Essay B"."""

            raw_out, in_tok = call_llm(prompt, max_tokens=10)
            essay_tokens += in_tok

            idx_t = id_map[t_id]
            idx_a = id_map[a_id]

            if "Essay A" in raw_out: comparisons.append((idx_t, idx_a))
            elif "Essay B" in raw_out: comparisons.append((idx_a, idx_t))

        latency_map[t_id] = time.time() - essay_start
        token_map[t_id] = essay_tokens

        if (idx+1) % 10 == 0: print(f"Processed {idx+1}/{len(target_df)}...")
        if PROVIDER != "MOCK": time.sleep(1)

    # 2. B-T è®¡ç®—
    if comparisons:
        try:
            params = choix.ilsr_pairwise(len(all_ids), comparisons, alpha=0.01)
            X_anc, y_anc = [], []
            for s, anchor in anchors.items():
                X_anc.append(params[id_map[anchor['id']]])
                y_anc.append(s)
            reg = LinearRegression().fit(np.array(X_anc).reshape(-1,1), y_anc)

            results = []
            for idx, row in target_df.iterrows():
                uid = row[id_col]
                latent = params[id_map[uid]]
                pred = int(round(max(1, min(6, reg.predict([[latent]])[0]))))
                results.append({
                    'essay_id': uid, 'human_score': row[score_col], 'pred_score': pred,
                    'input_tokens': token_map.get(uid, 0),
                    'latency': round(latency_map.get(uid, 0), 2)
                })
            save_results(results, "Baseline3_Pairwise_FullContext")
        except Exception as e:
            print(f"Modeling Error: {e}")
    else:
        print("No comparisons collected.")

if __name__ == "__main__":
    run()