# -*- coding: utf-8 -*-
"""run_e3_dsl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TUm5KkwcoW-TLHTh11VQWISTwroWi2VO
"""

# -*- coding: utf-8 -*-
"""
E³-DSL (Evidence → Evaluate → DSL-aggregate → Debias → Isotonic)
Self-contained pipeline with a *richer* MOCK that creates meaningful variation.

Upgrades vs previous version:
- Toy data with latent ground-truth traits (D*, O*, L*) and human score = 0.4D+0.3O+0.3L + noise.
- Essay generator injects evidence phrases / transitions / lexical signals per trait.
- MOCK Evidence Extractor parses the essay from the prompt and extracts *real* verbatim snippets.
- MOCK Scoring Officer bases YES/PARTIAL/NO on evidence presence and dimension heuristics.
- Dev raw QWK computed on (1 + 5*s_raw) rounded into 1..6 (no longer degenerate).
- Keep full artifact set identical to previous script.

Artifacts:
- rubric_checklist.json
- E3DSL_Dev_EvidenceLog_*.csv, E3DSL_Test_EvidenceLog_*.csv
- E3DSL_Dev_DecisionLog_*.csv, E3DSL_Test_DecisionLog_*.csv
- E3DSL_Dev_Data_*.csv, E3DSL_Test_Data_*.csv
- Contradiction_Candidates_*.csv
- E3DSL_Test_Report_*.txt
"""

import os, re, io, json, time, math, random, hashlib, warnings
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.linear_model import LinearRegression
from numpy import corrcoef

warnings.filterwarnings("ignore", category=UserWarning)

# =========================
# Config
# =========================
PROVIDER = "MOCK"               # "MOCK" / (real providers omitted in this self-contained file)
MODEL_NAME = "gemini-2.0-flash"
RANDOM_SEED = 42
DEV_SAMPLE = 64                 # larger dev to stabilize isotonic
TEST_SAMPLE = 16
E3_DSL_WEIGHTS = {"Development": 0.4, "Organization": 0.3, "Language": 0.3}
PROMPT_VARIANTS = {"evidence": ["v1", "v2"], "evaluate": ["v1", "v2"]}

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# =========================
# Helpers
# =========================
def ts():
    return time.strftime("%Y%m%d_%H%M%S")

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()[:10]

def clamp16(x: float) -> int:
    return int(max(1, min(6, round(float(x)))))

def json_extract(s: str) -> Any:
    m = re.findall(r'(\[.*\]|\{.*\})', s, flags=re.S)
    if not m:
        raise ValueError("No JSON found")
    return json.loads(m[-1])

def save_df_csv(df: pd.DataFrame, path: str):
    df.to_csv(path, index=False)
    print(f"Saved -> {path}")

def pcc_safe(x: np.ndarray, y: np.ndarray) -> float:
    if len(x) < 2 or np.std(x) == 0 or np.std(y) == 0:
        return float('nan')
    return float(corrcoef(x, y)[0,1])

# =========================
# Built-in rubric (fallback)
# =========================
BUILTIN_RUBRIC_TEXT = """
Holistic rubric (1-6) for short argumentative essays.
Dimensions: Development, Organization, Language.
- Development: ideas, evidence, reasoning, insight
- Organization: structure, coherence, transitions, focus
- Language: vocabulary, fluency, grammar/mechanics
"""

# =========================
# Phrase libraries for essay generation & evidence detection
# =========================
DEV_PHRASES = [
    "for example", "according to", "evidence shows", "because", "however", "counterargument",
    "the data suggest", "the source states", "on the contrary", "this implies"
]
ORG_PHRASES = [
    "firstly", "secondly", "therefore", "in conclusion", "moreover", "on the other hand",
    "furthermore", "to begin with", "as a result", "finally"
]
LANG_COMPLEX_WORDS = [
    "extraordinary", "comprehensive", "meticulous", "sophistication", "characterization",
    "incontrovertible", "counterproductive", "multidimensional", "idiosyncratic", "magnanimous"
]
LANG_COMMON_ERRORS = [
    "dont", "wont", "recieve", "teh", "it are", "grammer", "definately", "occured", "wich", "alot"
]

# =========================
# LLM shim
# =========================
def call_llm(prompt: str, max_tokens: int = 256, temperature: float = 0.0) -> Tuple[str, int, int]:
    """
    MOCK behavior upgraded:
      - Checklist Builder: 15 items (balanced across dims)
      - Evidence Extractor: parse essay text, pick shortest matching snippet per dimension (if any)
      - Scoring Officer: YES/PARTIAL/NO driven by evidence presence & dimension-specific heuristics
    """
    if PROVIDER == "MOCK":
        # Checklist builder
        if "Role: Checklist Builder" in prompt:
            dims = ["Development", "Organization", "Language"]
            items = []
            for i in range(15):
                dim = dims[i % 3]
                idx = f"{dim[:3].upper()}-{i+1:02d}"
                pos = ["supports claim with evidence", "clear linkage between ideas"]
                neg = ["irrelevant details", "unsupported assertions"]
                if dim == "Organization":
                    pos = ["uses transitions", "logical sequencing"]
                    neg = ["abrupt shifts", "missing transitions"]
                if dim == "Language":
                    pos = ["varied vocabulary", "few grammatical errors"]
                    neg = ["frequent errors", "imprecise wording"]
                items.append({
                    "id": idx,
                    "dimension": dim,
                    "criterion": f"{dim} Criterion {i+1}",
                    "operational_definition": f"Operationally judge {dim} using concrete textual evidence.",
                    "positive_indicators": pos,
                    "negative_indicators": neg
                })
            txt = json.dumps(items, ensure_ascii=False, indent=2)
            return txt, len(prompt)//4, len(txt)//4

        # Evidence extractor — parse essay and checklist; pick shortest matching phrase per item
        if "Role: Evidence Extractor" in prompt:
            essay = ""
            m_essay = re.search(r'ESSAY_START(.*?)ESSAY_END', prompt, re.S)
            if m_essay:
                essay = m_essay.group(1).strip().lower()

            m_chk = re.search(r'CHECKLIST_JSON:\s*(\[.*\])\s*ESSAY_START', prompt, re.S)
            checklist = []
            if m_chk:
                try:
                    checklist = json.loads(m_chk.group(1))
                except:
                    checklist = []

            def find_snippet(dimension: str) -> str:
                lib = DEV_PHRASES if dimension == "Development" else ORG_PHRASES if dimension == "Organization" else None
                if dimension == "Language":
                    # find a long word as evidence of lexical richness
                    words = re.findall(r"[a-z']{5,}", essay)
                    long_words = [w for w in words if len(w) >= 10]
                    if long_words:
                        return min(long_words, key=len)
                    # or show an error token (still verbatim evidence)
                    for err in LANG_COMMON_ERRORS:
                        if err in essay:
                            return err
                    return "N/A"
                if lib:
                    hits = [p for p in lib if p in essay]
                    if hits:
                        return min(hits, key=len)
                    return "N/A"
                return "N/A"

            out = []
            for it in checklist:
                evid = find_snippet(it.get("dimension",""))
                out.append({"id": it.get("id","UNK"), "evidence": evid})
            txt = json.dumps(out, ensure_ascii=False)
            return txt, len(prompt)//4, len(txt)//4

        # Scoring officer — base strictly on evidence presence & simple heuristics
        if "Role: Scoring Officer" in prompt:
            # Parse item defs
            m_defs = re.search(r'ITEM_DEFS:\s*(\[.*\])\s*EVIDENCE_JSON:', prompt, re.S)
            item_defs = []
            if m_defs:
                try:
                    item_defs = json.loads(m_defs.group(1))
                except:
                    item_defs = []
            id2dim = {d.get("id","UNK"): d.get("dimension","") for d in item_defs}

            # Parse evidence
            m_ev = re.search(r'EVIDENCE_JSON:\s*(\[.*\])\s*END_EVIDENCE', prompt, re.S)
            ev = []
            if m_ev:
                try:
                    ev = json.loads(m_ev.group(1))
                except:
                    ev = []

            decisions = []
            for rec in ev:
                iid = rec.get("id","UNK")
                evid = (rec.get("evidence","") or "").strip().lower()
                dim  = id2dim.get(iid, "")
                if evid == "n/a" or evid == "":
                    decision = "NO"
                    rationale = "no evidence"
                else:
                    # simple quality signal per dimension
                    if dim == "Language":
                        # complex long word → YES; error token → PARTIAL (not full credit)
                        if evid in LANG_COMMON_ERRORS:
                            decision = "PARTIAL"
                            rationale = "error token"
                        elif len(evid) >= 10:
                            decision = "YES"
                            rationale = "long word"
                        else:
                            decision = random.choice(["YES","PARTIAL"])
                            rationale = "lexical"
                    else:
                        # presence of legit transition/evidence phrase → mostly YES
                        decision = random.choices(["YES","PARTIAL"], weights=[0.8, 0.2], k=1)[0]
                        rationale = "snippet found"
                decisions.append({"id": iid, "decision": decision, "rationale": rationale[:25]})

            txt = json.dumps(decisions, ensure_ascii=False)
            return txt, len(prompt)//4, len(txt)//4

        # default
        text_out = "[]"
        return text_out, len(prompt)//4, len(text_out)//4

    raise NotImplementedError("Only MOCK is implemented in this self-contained script.")

# =========================
# Step 0: Build or load checklist
# =========================
def build_or_load_checklist(force_regen: bool=False,
                            rubric_text: str=BUILTIN_RUBRIC_TEXT,
                            out_path: str="rubric_checklist.json") -> Tuple[List[Dict[str,Any]], str]:
    if (not force_regen) and os.path.exists(out_path):
        with open(out_path, "r", encoding="utf-8") as f:
            items = json.load(f)
        return items, sha1(json.dumps(items, ensure_ascii=False))

    print("[Step0] Generating checklist via LLM ...")
    prompt = f"""Role: Checklist Builder
You are a K-12 writing assessment expert. Convert the following holistic rubric
into 12–18 operational checklist items across Development/Organization/Language.
Each item must be decisionable from verbatim essay text.

Rubric:
{rubric_text}

Output STRICT JSON array with fields:
"id","dimension","criterion","operational_definition",
"positive_indicators","negative_indicators"
"""
    out, _, _ = call_llm(prompt, max_tokens=1500)
    try:
        items = json_extract(out)
    except Exception:
        dims = ["Development","Organization","Language"]
        items = []
        for i in range(12):
            dim = dims[i%3]
            items.append({
                "id": f"{dim[:3].upper()}-{i+1:02d}",
                "dimension": dim,
                "criterion": f"Criterion {i+1}",
                "operational_definition": f"Operational definition for {dim} {i+1}",
                "positive_indicators": ["supports claim","logical link"],
                "negative_indicators": ["unsupported","irrelevant"]
            })
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)
    print(f"[Step0] Saved checklist -> {out_path} (n={len(items)})")
    return items, sha1(json.dumps(items, ensure_ascii=False))

# =========================
# Toy data with latent traits → essay text
# =========================
def generate_essay_text(d_star: int, o_star: int, l_star: int, base_len: int) -> str:
    """Compose a synthetic essay whose surface signals correlate with D/O/L."""
    pieces = []

    # Development evidence phrases (more if d_star high)
    dev_count = max(1, d_star // 2)
    dev_chunks = random.sample(DEV_PHRASES, k=min(dev_count, len(DEV_PHRASES)))
    for p in dev_chunks:
        pieces.append(f"{p}, the source states that the policy is effective.")

    # Organization transitions (more if o_star high)
    org_count = max(1, o_star // 2)
    org_chunks = random.sample(ORG_PHRASES, k=min(org_count, len(ORG_PHRASES)))
    for p in org_chunks:
        pieces.append(f"{p}, the next point builds upon the prior claim.")

    # Language signals
    lang_count = max(1, l_star // 2)
    if l_star >= 4:
        words = random.sample(LANG_COMPLEX_WORDS, k=min(lang_count, len(LANG_COMPLEX_WORDS)))
        for w in words:
            pieces.append(f"This argument demonstrates {w} reasoning.")
    else:
        errs = random.sample(LANG_COMMON_ERRORS, k=min(lang_count, len(LANG_COMMON_ERRORS)))
        for e in errs:
            pieces.append(f"People {e} understand, but it are okay sometimes.")

    # filler to reach approximate length
    filler = " This is a sentence."
    text = " ".join(pieces)
    need = max(0, base_len - len(text.split()))
    text += filler * (need // 3)

    return text.strip()

def make_toy_data(n: int = 80) -> pd.DataFrame:
    rows = []
    for i in range(n):
        # latent traits 1..6
        d = np.random.randint(1,7)
        o = np.random.randint(1,7)
        l = np.random.randint(1,7)

        # length grows with D/O (induces length bias)
        base_len = int(80 + 12*d + 8*o + np.random.normal(0, 12))
        base_len = max(60, min(280, base_len))

        txt = generate_essay_text(d, o, l, base_len)

        # human score from weighted traits + noise
        human = 0.4*d + 0.3*o + 0.3*l + np.random.normal(0, 0.4)
        human = int(max(1, min(6, round(human))))

        rows.append({
            "essay_id": f"E{i:04d}",
            "full_text": txt,
            "score": human,
            "gt_D": d, "gt_O": o, "gt_L": l,
            "len_chars": len(txt)
        })
    return pd.DataFrame(rows)

# =========================
# Step 1: Evidence Extraction
# =========================
def evidence_prompt(essay: str, checklist: List[Dict[str,Any]], variant_id: str) -> str:
    return f"""Role: Evidence Extractor
Given the essay and the checklist items, for each item return a JSON array of:
{{"id":"...","evidence":"<verbatim from essay or 'N/A'>"}}

Rules:
- Paste the shortest verbatim snippet (≤ 25 words) that supports/contradicts the item.
- If no relevant text exists, return "N/A".
- Do not invent or paraphrase unseen text.

CHECKLIST_JSON:
{json.dumps(checklist, ensure_ascii=False)}
ESSAY_START
{essay}
ESSAY_END
"""

def extract_evidence_for_one(essay_id: str,
                             essay: str,
                             checklist: List[Dict[str,Any]]) -> Tuple[Dict[str,Dict[str,Any]], List[Dict[str,Any]]]:
    all_variants_outputs: Dict[str, Dict[str,str]] = {}
    logs = []

    for v in PROMPT_VARIANTS["evidence"]:
        prompt = evidence_prompt(essay, checklist, v)
        t0 = time.time()
        out, tok_in, tok_out = call_llm(prompt, max_tokens=800)
        lat = (time.time()-t0)*1000
        try:
            arr = json_extract(out)
            if not isinstance(arr, list):
                arr = []
        except Exception:
            arr = []

        id2ev = {}
        for r in arr:
            iid = str(r.get("id","UNK"))
            ev = str(r.get("evidence","N/A"))
            id2ev[iid] = ev
            logs.append({
                "essay_id": essay_id,
                "item_id": iid,
                "variant": v,
                "evidence": ev,
                "input_tokens": tok_in,
                "output_tokens": tok_out,
                "latency_ms": round(lat,2),
                "raw_output": json.dumps(arr, ensure_ascii=False)[:2000]
            })
        # ensure coverage
        for it in checklist:
            iid = it["id"]
            if iid not in id2ev:
                id2ev[iid] = "N/A"

        for iid, ev in id2ev.items():
            all_variants_outputs.setdefault(iid, {})[v] = ev

    # merge: median on hit(1)/miss(0), choose shortest non-N/A as representative
    merged = {}
    for it in checklist:
        iid = it["id"]
        variants = all_variants_outputs.get(iid, {})
        evs = list(variants.values())
        xs = [0 if (str(e).strip().lower()=="n/a") else 1 for e in evs] or [0]
        dispersion = float(np.max(xs)-np.min(xs)) if xs else 0.0
        chosen = "N/A"
        nonna = [e for e in evs if str(e).strip().lower()!="n/a"]
        if nonna:
            chosen = min(nonna, key=len)
        merged[iid] = {"evidence": chosen, "dispersion": dispersion}

    # fallback: ensure at least one non-N/A
    if all((v["evidence"].strip().lower()=="n/a") for v in merged.values()):
        first_key = next(iter(merged.keys()))
        merged[first_key] = {"evidence": "for example", "dispersion": 0.0}

    return merged, logs

# =========================
# Step 2: Evaluate Decisions
# =========================
def evaluate_prompt(evidence_recs: List[Dict[str,Any]],
                    items: List[Dict[str,Any]],
                    variant_id: str) -> str:
    return f"""Role: Scoring Officer
Using ONLY the provided evidence and item definitions, output JSON array:
{{"id":"...","decision":"YES|PARTIAL|NO","rationale":"≤25 chars"}}

Constraints:
- If evidence is "N/A", default to NO or PARTIAL according to the item's minimal requirement.
- Do not introduce new evidence.

ITEM_DEFS:
{json.dumps(items, ensure_ascii=False)}

EVIDENCE_JSON:
{json.dumps(evidence_recs, ensure_ascii=False)}
END_EVIDENCE
"""

def judge_decisions_for_one(essay_id: str,
                            evidence_by_id: Dict[str,Dict[str,Any]],
                            checklist: List[Dict[str,Any]]) -> Tuple[Dict[str,Dict[str,Any]], List[Dict[str,Any]]]:
    item_map = {it["id"]: it for it in checklist}
    all_variants: Dict[str, List[str]] = {}
    logs = []

    ev_list = [{"id": iid, "evidence": info["evidence"]} for iid, info in evidence_by_id.items()]

    for v in PROMPT_VARIANTS["evaluate"]:
        prompt = evaluate_prompt(ev_list, checklist, v)
        t0 = time.time()
        out, tok_in, tok_out = call_llm(prompt, max_tokens=600)
        lat = (time.time()-t0)*1000
        try:
            arr = json_extract(out)
            if not isinstance(arr, list):
                arr = []
        except Exception:
            arr = []

        norm = []
        for r in arr:
            iid = str(r.get("id","UNK"))
            dec = str(r.get("decision","NO")).upper()
            if dec not in {"YES","PARTIAL","NO"}: dec = "NO"
            rat = str(r.get("rationale",""))[:25]
            norm.append({"id": iid, "decision": dec, "rationale": rat})
            all_variants.setdefault(iid, []).append(dec)

        have_ids = {n["id"] for n in norm}
        for iid in item_map.keys():
            if iid not in have_ids:
                all_variants.setdefault(iid, []).append("NO")
                norm.append({"id": iid, "decision": "NO", "rationale": "missing"})

        for r in norm:
            logs.append({
                "essay_id": essay_id,
                "item_id": r["id"],
                "variant": v,
                "decision": r["decision"],
                "rationale": r["rationale"],
                "input_tokens": tok_in,
                "output_tokens": tok_out,
                "latency_ms": round(lat,2),
                "raw_output": json.dumps(norm, ensure_ascii=False)[:2000]
            })

    # merge
    mp = {"YES":1.0,"PARTIAL":0.5,"NO":0.0}
    merged = {}
    for iid, dlist in all_variants.items():
        xs = [mp.get(d,0.0) for d in dlist]
        median_val = float(np.median(xs))
        disp = float(np.max(xs) - np.min(xs)) if xs else 0.0
        # majority label
        counts = {"YES":0,"PARTIAL":0,"NO":0}
        for d in dlist: counts[d] = counts.get(d,0)+1
        label = max(counts.items(), key=lambda kv: kv[1])[0]
        merged[iid] = {"decision": label, "score": median_val, "dispersion": disp}

    return merged, logs

# =========================
# Step 3: Aggregate → trait scores and s_raw
# =========================
def aggregate_scores(merged_decisions: Dict[str,Dict[str,Any]],
                     checklist: List[Dict[str,Any]]) -> Dict[str, float]:
    dim2scores = {"Development": [], "Organization": [], "Language": []}
    dim2na = {"Development":0, "Organization":0, "Language":0}
    dim2total = {"Development":0, "Organization":0, "Language":0}

    for it in checklist:
        iid = it["id"]; dim = it["dimension"]
        dim2total[dim] += 1
        rec = merged_decisions.get(iid, {"score":0.0})
        sc = float(rec.get("score",0.0))
        dim2scores[dim].append(sc)
        if sc == 0.0:
            dim2na[dim] += 1

    dev = float(np.mean(dim2scores["Development"])) if dim2scores["Development"] else 0.0
    org = float(np.mean(dim2scores["Organization"])) if dim2scores["Organization"] else 0.0
    lan = float(np.mean(dim2scores["Language"])) if dim2scores["Language"] else 0.0

    s_raw = (E3_DSL_WEIGHTS["Development"]*dev +
             E3_DSL_WEIGHTS["Organization"]*org +
             E3_DSL_WEIGHTS["Language"]*lan)

    na_rate = (dim2na["Development"]+dim2na["Organization"]+dim2na["Language"]) / \
              max(1,(dim2total["Development"]+dim2total["Organization"]+dim2total["Language"]))
    return {"D":dev, "O":org, "L":lan, "s_raw": s_raw, "na_rate": float(na_rate)}

# =========================
# Step 4: Debias (length) + Isotonic
# =========================
@dataclass
class CalibModel:
    beta: float
    mu_loglen: float
    iso_x: np.ndarray
    iso_y: np.ndarray
    iso: IsotonicRegression

def fit_calibration(dev_df: pd.DataFrame) -> CalibModel:
    loglen = np.log(dev_df["len_chars"].to_numpy()+1.0)
    y = dev_df["human_score"].to_numpy().astype(float)
    mu = float(np.mean(loglen))
    lr = LinearRegression().fit(loglen.reshape(-1,1), y)
    beta = float(lr.coef_[0])

    s_len = dev_df["s_raw"].to_numpy().astype(float) - beta*(loglen - mu)

    iso = IsotonicRegression(y_min=1.0, y_max=6.0, increasing=True, out_of_bounds="clip")
    iso.fit(s_len, y)

    return CalibModel(beta=beta, mu_loglen=mu, iso_x=s_len, iso_y=y, iso=iso)

def apply_calibration(df: pd.DataFrame, cm: CalibModel) -> np.ndarray:
    loglen = np.log(df["len_chars"].to_numpy()+1.0)
    s_len = df["s_raw"].to_numpy().astype(float) - cm.beta*(loglen - cm.mu_loglen)
    y_iso = cm.iso.predict(s_len)
    return y_iso

# =========================
# Step 5/6 utilities
# =========================
def detect_conflicts(decision_logs: pd.DataFrame, threshold: float=1.0) -> pd.DataFrame:
    mp = {"YES":1.0,"PARTIAL":0.5,"NO":0.0}
    g = decision_logs.groupby(["essay_id","item_id"])
    rows = []
    for (eid, iid), sub in g:
        xs = [mp.get(d,0.0) for d in sub["decision"].tolist()]
        disp = float(np.max(xs)-np.min(xs)) if xs else 0.0
        if disp > threshold:
            rows.append({
                "essay_id": eid,
                "item_id": iid,
                "decisions_by_variant": ";".join(sub["decision"].astype(str).tolist()),
                "dispersion": disp
            })
    return pd.DataFrame(rows)

# =========================
# Main
# =========================
def main():
    print("[Rubric] Using default built-in text")
    checklist, chk_id = build_or_load_checklist(force_regen=False)
    print(f"[Checklist] id={chk_id}, n={len(checklist)}")

    # data
    df = make_toy_data(n=DEV_SAMPLE + TEST_SAMPLE + 24)  # a bit extra
    df = df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)
    df_dev = df.iloc[:DEV_SAMPLE].copy()
    df_test = df.iloc[DEV_SAMPLE:DEV_SAMPLE+TEST_SAMPLE].copy()
    print(f"[Data] dev={len(df_dev)} test={len(df_test)} (total {len(df)})")

    tstamp = ts()
    ev_dev_rows, ev_test_rows = [], []
    dc_dev_rows, dc_test_rows = [], []

    # -------- DEV pipeline --------
    print("[DEV] Evidence → Evaluate → Aggregate ...")
    dev_records = []
    for _, r in df_dev.iterrows():
        evid_by_id, ev_logs = extract_evidence_for_one(r["essay_id"], r["full_text"], checklist)
        ev_dev_rows.extend(ev_logs)

        dec_by_id, dc_logs = judge_decisions_for_one(r["essay_id"], evid_by_id, checklist)
        dc_dev_rows.extend(dc_logs)

        agg = aggregate_scores(dec_by_id, checklist)
        dev_records.append({
            "essay_id": r["essay_id"],
            "human_score": int(r["score"]),
            "len_chars": len(r["full_text"]),
            **agg,
            "gt_D": r["gt_D"], "gt_O": r["gt_O"], "gt_L": r["gt_L"]
        })
    dev_df = pd.DataFrame(dev_records)

    print("[DEV] Fit debias + isotonic ...")
    cm = fit_calibration(dev_df)

    # dev metrics
    y_true_dev = dev_df["human_score"].to_numpy().astype(int)
    y_raw_dev_scaled = np.rint(1 + 5*dev_df["s_raw"].to_numpy()).clip(1,6).astype(int)
    qwk_raw = cohen_kappa_score(y_true_dev, y_raw_dev_scaled, weights="quadratic")
    y_iso_dev = apply_calibration(dev_df, cm)
    qwk_iso_dev = cohen_kappa_score(y_true_dev, np.rint(y_iso_dev).clip(1,6).astype(int), weights="quadratic")
    print(f"[Step4/Dev] QWK raw={qwk_raw:.4f} | isotonic={qwk_iso_dev:.4f}")

    # -------- TEST pipeline --------
    print("[TEST] Evidence → Evaluate → Aggregate ...")
    test_records = []
    for _, r in df_test.iterrows():
        evid_by_id, ev_logs = extract_evidence_for_one(r["essay_id"], r["full_text"], checklist)
        ev_test_rows.extend(ev_logs)
        dec_by_id, dc_logs = judge_decisions_for_one(r["essay_id"], evid_by_id, checklist)
        dc_test_rows.extend(dc_logs)
        agg = aggregate_scores(dec_by_id, checklist)
        test_records.append({
            "essay_id": r["essay_id"],
            "human_score": int(r["score"]),
            "len_chars": len(r["full_text"]),
            **agg,
            "gt_D": r["gt_D"], "gt_O": r["gt_O"], "gt_L": r["gt_L"]
        })
    test_df = pd.DataFrame(test_records)
    y_true = test_df["human_score"].to_numpy().astype(int)
    y_iso = apply_calibration(test_df, cm)
    y_pred_final = np.array([clamp16(y) for y in y_iso])

    # -------- Save artifacts --------
    ev_dev = pd.DataFrame(ev_dev_rows); ev_test = pd.DataFrame(ev_test_rows)
    dc_dev = pd.DataFrame(dc_dev_rows); dc_test = pd.DataFrame(dc_test_rows)
    dev_out = dev_df.copy(); test_out = test_df.copy()
    test_out["pred_iso"] = y_iso
    test_out["pred"] = y_pred_final

    ev_dev_path = f"E3DSL_Dev_EvidenceLog_{tstamp}.csv"
    ev_test_path = f"E3DSL_Test_EvidenceLog_{tstamp}.csv"
    dc_dev_path = f"E3DSL_Dev_DecisionLog_{tstamp}.csv"
    dc_test_path = f"E3DSL_Test_DecisionLog_{tstamp}.csv"
    dev_data_path = f"E3DSL_Dev_Data_{tstamp}.csv"
    test_data_path = f"E3DSL_Test_Data_{tstamp}.csv"

    save_df_csv(ev_dev, ev_dev_path)
    save_df_csv(ev_test, ev_test_path)
    save_df_csv(dc_dev, dc_dev_path)
    save_df_csv(dc_test, dc_test_path)
    save_df_csv(dev_out, dev_data_path)
    save_df_csv(test_out, test_data_path)

    conflicts = detect_conflicts(dc_test)
    conf_path = f"Contradiction_Candidates_{tstamp}.csv"
    save_df_csv(conflicts, conf_path)

    # -------- Report --------
    qwk = cohen_kappa_score(y_true, y_pred_final, weights="quadratic")
    D_pcc = pcc_safe(test_out["D"].to_numpy(), y_true)
    O_pcc = pcc_safe(test_out["O"].to_numpy(), y_true)
    L_pcc = pcc_safe(test_out["L"].to_numpy(), y_true)
    resid = y_true - y_iso
    corr_resid_len = pcc_safe(resid, test_out["len_chars"].to_numpy())
    all_na_rate = float(np.mean(test_out["na_rate"].to_numpy()>=0.99))

    report = f"""=== E³-DSL Test Report ({tstamp}) ===
Provider: {PROVIDER} | Model: {MODEL_NAME}
Checklist id: {chk_id} (n={len(checklist)})
Data: dev={len(dev_df)} test={len(test_df)}

[Main Metric]
QWK (rounded isotonic) = {qwk:.4f}

[Aux]
Pearson(D, human) = {D_pcc if not np.isnan(D_pcc) else 'nan'}
Pearson(O, human) = {O_pcc if not np.isnan(O_pcc) else 'nan'}
Pearson(L, human) = {L_pcc if not np.isnan(L_pcc) else 'nan'}
Corr(len_chars, residual) = {corr_resid_len if not np.isnan(corr_resid_len) else 'nan'}

[Health/Cost]
All-N/A evidence rate (test) = {all_na_rate:.3f}
High-dispersion rate (test)   = {float(np.mean(conflicts['dispersion']>1.0)) if len(conflicts)>0 else 0.0:.3f}

Artifacts:
 - Evidence Logs: {ev_dev_path}, {ev_test_path}
 - Decision Logs: {dc_dev_path}, {dc_test_path}
 - Data CSV:      {dev_data_path}, {test_data_path}
 - Conflicts:     {conf_path}
"""
    rep_path = f"E3DSL_Test_Report_{tstamp}.txt"
    with open(rep_path, "w", encoding="utf-8") as f:
        f.write(report)
    print(report)

if __name__ == "__main__":
    main()